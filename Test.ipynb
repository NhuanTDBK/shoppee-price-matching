{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU found\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "if tf.test.gpu_device_name():\n",
    "    print('GPU found')\n",
    "else:\n",
    "    print(\"No GPU found\")\n",
    "\n",
    "from typing import Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import transformers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from modelling.models import TextProductMatch\n",
    "from sklearn.model_selection import KFold\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "import os\n",
    "\n",
    "params = {\n",
    "    \"N_CLASSES\": 11014,\n",
    "    \"MAX_LEN\": 20,\n",
    "    \"MODEL_NAME\": 'bert-base-multilingual-uncased',\n",
    "    \"POOLING\": \"global_avg_1d\",\n",
    "    \"EPOCHS\": 5,\n",
    "    \"BATCH_SIZE\": 16,\n",
    "    \"METRIC\": \"adacos\"\n",
    "}\n",
    "PATH_NAME = 'saved/arcface/v1'\n",
    "os.makedirs(PATH_NAME,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelling.metrics import *\n",
    "from modelling.pooling import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-multilingual-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "config = transformers.BertConfig.from_pretrained(params[\"MODEL_NAME\"])\n",
    "config.output_hidden_states = True\n",
    "word_model = transformers.TFAutoModel.from_pretrained(params[\"MODEL_NAME\"],config=config)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(params[\"MODEL_NAME\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"N_CLASSES\": 11014,\n",
    "    \"MAX_LEN\": 70,\n",
    "    \"MODEL_NAME\": 'bert-base-multilingual-uncased',\n",
    "    \"POOLING\": \"global_avg_1d\",\n",
    "    \"EPOCHS\": 5,\n",
    "    \"BATCH_SIZE\": 32,\n",
    "    \"METRIC\": \"adacos\",\n",
    "    \"LAST_HIDDEN_STATES\": 3 \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fa2c41423a0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fa2c41423a0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 70)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 70)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 70)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     TFBaseModelOutputWit 167356416   input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_stack (TensorFlowOp [(3, None, 70, 768)] 0           tf_bert_model[0][13]             \n",
      "                                                                 tf_bert_model[0][11]             \n",
      "                                                                 tf_bert_model[0][10]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mean (TensorFlowOpL [(None, 70, 768)]    0           tf_op_layer_stack[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Max (TensorFlowOpLa [(None, 70, 768)]    0           tf_op_layer_stack[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat (TensorFlowO [(None, 70, 1536)]   0           tf_op_layer_Mean[0][0]           \n",
      "                                                                 tf_op_layer_Max[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 11014)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "text_product_match (TextProduct (None, 11014)        6428161     tf_op_layer_concat[0][0]         \n",
      "                                                                 input_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 173,784,577\n",
      "Trainable params: 173,783,552\n",
      "Non-trainable params: 1,025\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "ids = tf.keras.layers.Input((params[\"MAX_LEN\"],), dtype=tf.int32)\n",
    "att = tf.keras.layers.Input((params[\"MAX_LEN\"],), dtype=tf.int32)\n",
    "tok = tf.keras.layers.Input((params[\"MAX_LEN\"],), dtype=tf.int32)\n",
    "\n",
    "labels_onehot = tf.keras.layers.Input(shape=(params[\"N_CLASSES\"]), dtype=tf.int32)\n",
    "\n",
    "x = word_model(ids, attention_mask=att, token_type_ids=tok)[-1]\n",
    "x1 = tf.stack([x[-i-1] for i in range(params[\"LAST_HIDDEN_STATES\"])])\n",
    "x1_mean = tf.math.reduce_mean(x1, axis=0)\n",
    "x1_max = tf.math.reduce_max(x1, axis=0)\n",
    "\n",
    "x1 = tf.concat([x1_mean, x1_max],axis=-1)\n",
    "\n",
    "x1 = TextProductMatch(params[\"N_CLASSES\"],\n",
    "                    params[\"POOLING\"],\n",
    "                    metric=params[\"METRIC\"],\n",
    "                    use_fc=True)([x1, labels_onehot])\n",
    "\n",
    "model = tf.keras.Model(inputs=[[ids, att, tok], labels_onehot], outputs=[x1])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ada_cos_3/cosine_similarity_3/MatMul:0' shape=(None, 1000) dtype=float32>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'global_average_pooling1d/Mean:0' shape=(None, 1536) dtype=float32>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
